{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete: libraries imported, random seed set, and tickers defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_45396\\480519525.py:22: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-darkgrid')\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data handling, model utilization, and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf  # For collecting financial data\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from scipy.optimize import minimize \n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "# Import the custom Model class\n",
    "from Model import Model\n",
    "\n",
    "# Set the random seed for reproducibility across numpy and tensorflow\n",
    "np.random.seed(123)\n",
    "\n",
    "# Optional: Configure plotting style for a consistent look\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Define the tickers and date range with consideration of trading days\n",
    "TICKERS = ['VTI','AGG','DBC','VIXY']  # Example tickers for the portfolio\n",
    "START_DATE = '2018-01-01'\n",
    "END_DATE = '2023-01-01'\n",
    "\n",
    "# Approximate number of trading days per year (useful for annualizing returns)\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "# Confirm setup\n",
    "print(\"Setup complete: libraries imported, random seed set, and tickers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  4 of 4 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched successfully. Sample data:\n",
      "Ticker            AGG        DBC        VIXY         VTI\n",
      "Date                                                    \n",
      "2018-01-02  91.025757  15.363913  446.600006  123.725113\n",
      "2018-01-03  91.034103  15.455968  437.799988  124.441193\n",
      "2018-01-04  90.975708  15.437557  436.399994  124.915642\n",
      "2018-01-05  90.917366  15.382323  436.600006  125.649628\n",
      "2018-01-08  90.892303  15.373118  431.000000  125.936081\n",
      "Data covers 1259 trading days with 4 assets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Collection Step\n",
    "# Objective: Fetch historical adjusted close prices for defined tickers and date range\n",
    "\n",
    "# Download data using yfinance for the specified tickers and date range\n",
    "def get_data(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Retrieves historical adjusted close prices for the given tickers and date range.\n",
    "    \n",
    "    Parameters:\n",
    "    - tickers: List of stock ticker symbols\n",
    "    - start_date: Start date for historical data\n",
    "    - end_date: End date for historical data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame of adjusted close prices, with each column representing a ticker\n",
    "    \"\"\"\n",
    "    # Fetch data from yfinance\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "    \n",
    "    # Drop rows with missing values, if any, to ensure data continuity\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Fetch the data and display a quick preview\n",
    "data = get_data(TICKERS, START_DATE, END_DATE)\n",
    "print(\"Data fetched successfully. Sample data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Confirm data spans the expected range and has the expected number of columns\n",
    "print(f\"Data covers {len(data)} trading days with {len(data.columns)} assets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete. Sample normalized data:\n",
      "Ticker           AGG       DBC      VIXY       VTI\n",
      "Date                                              \n",
      "2018-01-02  1.000000  1.000000  1.000000  1.000000\n",
      "2018-01-03  1.000092  1.005992  0.980296  1.005788\n",
      "2018-01-04  0.999450  1.004793  0.977161  1.009622\n",
      "2018-01-05  0.998809  1.001198  0.977609  1.015555\n",
      "2018-01-08  0.998534  1.000599  0.965069  1.017870\n",
      "\n",
      "Sample daily returns:\n",
      "Ticker           AGG       DBC      VIXY       VTI\n",
      "Date                                              \n",
      "2018-01-03  0.000092  0.005992 -0.019704  0.005788\n",
      "2018-01-04 -0.000641 -0.001191 -0.003198  0.003813\n",
      "2018-01-05 -0.000641 -0.003578  0.000458  0.005876\n",
      "2018-01-08 -0.000276 -0.000598 -0.012826  0.002280\n",
      "2018-01-09 -0.002752  0.006587  0.010209  0.001848\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing Step\n",
    "# Objective: Prepare data by calculating daily returns and normalizing for model input\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Prepares data for the LSTM model by calculating returns and normalizing prices.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame of historical adjusted close prices for assets\n",
    "\n",
    "    Returns:\n",
    "    - normalized_data: DataFrame with prices normalized to start at 1 for each asset\n",
    "    - returns: DataFrame with daily returns for each asset\n",
    "    \"\"\"\n",
    "    # Calculate daily returns as percentage changes\n",
    "    returns = data.pct_change().dropna()\n",
    "    \n",
    "    # Normalize prices so each series starts at 1\n",
    "    normalized_data = data / data.iloc[0]\n",
    "    \n",
    "    return normalized_data, returns\n",
    "\n",
    "# Run preprocessing and display sample data\n",
    "normalized_data, returns = preprocess_data(data)\n",
    "print(\"Data preprocessing complete. Sample normalized data:\")\n",
    "print(normalized_data.head())\n",
    "print(\"\\nSample daily returns:\")\n",
    "print(returns.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Portfolio:\n",
    "    def __init__(self, initial_cash: float, assets: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the Portfolio object.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_cash: The starting value of the portfolio in cash.\n",
    "        - assets: DataFrame of asset prices (historical data).\n",
    "        \"\"\"\n",
    "        self.initial_cash = initial_cash\n",
    "        self.current_value = initial_cash\n",
    "        self.assets = assets  # Historical price data for the assets\n",
    "        self.weights = np.zeros(len(assets.columns))  # Initialize weights as zero\n",
    "        self.portfolio_history = []  # To track portfolio value over time\n",
    "        self.rebalancing_dates = []  # To store rebalancing dates\n",
    "\n",
    "    def rebalance(self, new_weights: np.array):\n",
    "        \"\"\"\n",
    "        Rebalances the portfolio according to new weights.\n",
    "\n",
    "        Parameters:\n",
    "        - new_weights: Numpy array of asset allocations.\n",
    "        \"\"\"\n",
    "        if len(new_weights) != len(self.assets.columns):\n",
    "            raise ValueError(\"Number of weights must match the number of assets.\")\n",
    "        self.weights = new_weights\n",
    "\n",
    "    def calculate_initial_shares(self, initial_cash, initial_prices):\n",
    "        \"\"\"\n",
    "        Calculates the number of shares for each asset at the start of the testing period based on\n",
    "        initial cash and allocation weights.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_cash: The starting cash value of the portfolio.\n",
    "\n",
    "        Returns:\n",
    "        - shares: Dictionary with tickers as keys and the initial number of shares as values.\n",
    "        \"\"\"\n",
    "        # Calculate the dollar amount allocated to each asset\n",
    "        dollar_allocation = initial_cash * self.weights\n",
    "\n",
    "        # Calculate the number of shares for each asset\n",
    "        shares = (dollar_allocation // initial_prices).astype(int)  # Floor division to get whole shares\n",
    "\n",
    "        # Return as a dictionary for easy readability\n",
    "        return dict(zip(self.assets.columns, shares))\n",
    "    \n",
    "    def calculate_daily_returns(self):\n",
    "        \"\"\"\n",
    "        Applies the current weights to asset returns and updates portfolio value over time.\n",
    "        \"\"\"\n",
    "        # Calculate daily returns for each asset\n",
    "        daily_returns = self.assets.pct_change().dropna()\n",
    "        \n",
    "        # Calculate portfolio returns by applying weights\n",
    "        portfolio_returns = np.dot(daily_returns, self.weights)\n",
    "\n",
    "        # Track the portfolio's value over time by compounding the returns\n",
    "        for daily_ret in portfolio_returns:\n",
    "            self.current_value *= (1 + daily_ret)\n",
    "            self.portfolio_history.append(self.current_value)\n",
    "\n",
    "    def track_portfolio_performance(self):\n",
    "        \"\"\"\n",
    "        Tracks and prints the portfolio performance over time.\n",
    "        \"\"\"\n",
    "        for date, value in zip(self.assets.index[1:], self.portfolio_history):\n",
    "            print(f\"Date: {date}, Portfolio Value: {value}\")\n",
    "\n",
    "    def get_portfolio_value(self):\n",
    "        \"\"\"\n",
    "        Returns the current value of the portfolio.\n",
    "        \"\"\"\n",
    "        return self.current_value\n",
    "    \n",
    "    def plot_portfolio_value(self):\n",
    "        \"\"\"\n",
    "        Plots the portfolio value over time.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.assets.index[1:], self.portfolio_history, label=\"Portfolio Value\")\n",
    "        plt.title(\"Portfolio Value Over Time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Portfolio Value\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_rebalance_portfolio(portfolio: Portfolio, frequency: int, model: Model):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model and rebalances the portfolio at the end of each specified interval.\n",
    "\n",
    "    Parameters:\n",
    "    - portfolio: An instance of the Portfolio class.\n",
    "    - frequency: The frequency of rebalancing in days.\n",
    "    - model: An instance of the Model class for training.\n",
    "    \"\"\"\n",
    "    num_days = len(portfolio.assets)\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < num_days:\n",
    "        # Determine the end index for the current training period\n",
    "        end_index = min(start_index + frequency, num_days)\n",
    "\n",
    "        # Slice the data for the training period\n",
    "        train_data = portfolio.assets.iloc[start_index:end_index]\n",
    "\n",
    "        # Preprocess the training data\n",
    "        normalized_data, _ = preprocess_data(train_data)\n",
    "\n",
    "        # Get allocations from the model\n",
    "        allocations = model.get_allocations(normalized_data)\n",
    "\n",
    "        # Rebalance the portfolio with the optimized weights\n",
    "        portfolio.rebalance(allocations)\n",
    "\n",
    "        # Store the rebalancing date\n",
    "        portfolio.rebalancing_dates.append(train_data.index[-1])\n",
    "\n",
    "        # Move to the next period\n",
    "        start_index += frequency\n",
    "    return allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(portfolio_values):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics for the portfolio.\n",
    "\n",
    "    Parameters:\n",
    "    - portfolio_values: List of daily portfolio values over the testing period.\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary containing Sharpe Ratio, Sortino Ratio, and Max Drawdown.\n",
    "    \"\"\"\n",
    "    # Convert portfolio values to daily returns\n",
    "    portfolio_returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "    \n",
    "    # Calculate Sharpe Ratio\n",
    "    mean_return = np.mean(portfolio_returns)\n",
    "    std_dev = np.std(portfolio_returns)\n",
    "    sharpe_ratio = mean_return / std_dev * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    \n",
    "    # Calculate Sortino Ratio\n",
    "    downside_returns = portfolio_returns[portfolio_returns < 0]\n",
    "    downside_std_dev = np.std(downside_returns)\n",
    "    sortino_ratio = mean_return / downside_std_dev * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    \n",
    "    # Calculate Maximum Drawdown\n",
    "    cumulative_returns = np.maximum.accumulate(portfolio_values) - portfolio_values\n",
    "    max_drawdown = np.max(cumulative_returns / np.maximum.accumulate(portfolio_values))\n",
    "    \n",
    "    return {\n",
    "        \"Sharpe Ratio\": sharpe_ratio,\n",
    "        \"Sortino Ratio\": sortino_ratio,\n",
    "        \"Max Drawdown\": max_drawdown\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_weighted_strategy(returns):\n",
    "    \"\"\"\n",
    "    Creates an equal-weighted portfolio.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: DataFrame of daily returns for each asset.\n",
    "\n",
    "    Returns:\n",
    "    - equal_weights: Numpy array of equal weights for each asset.\n",
    "    \"\"\"\n",
    "    num_assets = returns.shape[1]\n",
    "    equal_weights = np.ones(num_assets) / num_assets\n",
    "    return equal_weights\n",
    "\n",
    "def mean_variance_optimized_strategy(returns):\n",
    "    \"\"\"\n",
    "    Creates a mean-variance optimized portfolio by maximizing the Sharpe Ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: DataFrame of daily returns for each asset.\n",
    "\n",
    "    Returns:\n",
    "    - optimized_weights: Numpy array of optimized weights for each asset.\n",
    "    \"\"\"\n",
    "    mean_returns = returns.mean()\n",
    "    cov_matrix = returns.cov()\n",
    "    \n",
    "    def neg_sharpe(weights):\n",
    "        portfolio_return = np.dot(weights, mean_returns)\n",
    "        portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        return -portfolio_return / portfolio_std\n",
    "\n",
    "    # Constraints: Weights must sum to 1, and each weight must be between 0 and 1\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(returns.shape[1]))\n",
    "\n",
    "    result = minimize(neg_sharpe, np.ones(returns.shape[1]) / returns.shape[1], bounds=bounds, constraints=constraints)\n",
    "    optimized_weights = result.x\n",
    "    \n",
    "    return optimized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation for Training and Testing\n",
    "train_data = get_data(TICKERS, START_DATE, END_DATE)\n",
    "normalized_train_data, _ = preprocess_data(train_data)\n",
    "\n",
    "testing_data = get_data(TICKERS, '2023-01-01', '2024-01-01')\n",
    "normalized_testing_data, testing_returns = preprocess_data(testing_data)\n",
    "\n",
    "# Step 2: Train and Rebalance Portfolio in Training Phase (rebalancing every two years)\n",
    "training_portfolio = Portfolio(100000, normalized_train_data)\n",
    "model = Model()\n",
    "initial_prices = testing_data.loc[testing_data.index[0]]  # First row of the actual price data\n",
    "# Rebalance every 2 years (252 trading days * 2)\n",
    "final_allocations = train_and_rebalance_portfolio(training_portfolio, frequency=30, model=model)\n",
    "\n",
    "# Step 3: Initialize Testing Portfolio with Final LSTM Weights\n",
    "testing_portfolio = Portfolio(100000, normalized_testing_data)\n",
    "testing_portfolio.rebalance(final_allocations)\n",
    "LSTM_shares = testing_portfolio.calculate_initial_shares(100000,initial_prices)  # Calculate initial shares based on final allocations\n",
    "testing_portfolio.calculate_daily_returns()     # Track performance in testing period\n",
    "testing_portfolio.plot_portfolio_value()        # Visualize portfolio value over time\n",
    "# Step 4: Calculate Performance Metrics for LSTM Portfolio\n",
    "metrics = calculate_metrics(testing_portfolio.portfolio_history)\n",
    "\n",
    "# Step 5: Baseline Strategies Setup and Evaluation\n",
    "# Equal-Weighted and Mean-Variance Optimized Portfolios\n",
    "equal_weights = equal_weighted_strategy(testing_returns)\n",
    "mv_optimized_weights = mean_variance_optimized_strategy(testing_returns)\n",
    "\n",
    "equal_weight_portfolio = Portfolio(100000, normalized_testing_data)\n",
    "mv_optimized_portfolio = Portfolio(100000, normalized_testing_data)\n",
    "\n",
    "equal_weight_portfolio.rebalance(equal_weights)\n",
    "mv_optimized_portfolio.rebalance(mv_optimized_weights)\n",
    "\n",
    "equal_weight_portfolio.calculate_daily_returns()\n",
    "mv_optimized_portfolio.calculate_daily_returns()\n",
    "ew_shares = equal_weight_portfolio.calculate_initial_shares(100000,initial_prices)\n",
    "mv_shares = mv_optimized_portfolio.calculate_initial_shares(100000,initial_prices)\n",
    "equal_weight_portfolio.plot_portfolio_value()\n",
    "mv_optimized_portfolio.plot_portfolio_value()\n",
    "# Step 6: Calculate Performance Metrics for Baseline Portfolios\n",
    "equal_weight_metrics = calculate_metrics(equal_weight_portfolio.portfolio_history)\n",
    "mv_optimized_metrics = calculate_metrics(mv_optimized_portfolio.portfolio_history)\n",
    "\n",
    "print(\"LSTM Portfolio Metrics:\")\n",
    "print(f\"Weights:{final_allocations}, Shares:{LSTM_shares}\")\n",
    "print(metrics)\n",
    "print(\"\\nEqual-Weighted Portfolio Metrics:\")\n",
    "print(f\"Weights:{equal_weights}, Shares:{ew_shares}\")\n",
    "print(equal_weight_metrics)\n",
    "\n",
    "print(\"\\nMean-Variance Optimized Portfolio Metrics:\")\n",
    "print(f\"Weights:{mv_optimized_weights}, Shares:{mv_shares}\")\n",
    "print(mv_optimized_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison of performance with different training frequencies\n",
    "model2 = Model()\n",
    "training_portfolio = Portfolio(100000, normalized_train_data)\n",
    "initial_prices = testing_data.loc[testing_data.index[0]]  # First row of the actual price data\n",
    "# Rebalance every 2 years (252 trading days * 2)\n",
    "final_allocations = train_and_rebalance_portfolio(training_portfolio, frequency=15, model=model2)\n",
    "\n",
    "# Step 3: Initialize Testing Portfolio with Final LSTM Weights\n",
    "testing_portfolio = Portfolio(100000, normalized_testing_data)\n",
    "testing_portfolio.rebalance(final_allocations)\n",
    "LSTM_shares = testing_portfolio.calculate_initial_shares(100000,initial_prices)  # Calculate initial shares based on final allocations\n",
    "testing_portfolio.calculate_daily_returns()     # Track performance in testing period\n",
    "testing_portfolio.plot_portfolio_value()        # Visualize portfolio value over time\n",
    "# Step 4: Calculate Performance Metrics for LSTM Portfolio\n",
    "metrics = calculate_metrics(testing_portfolio.portfolio_history)\n",
    "print(\"LSTM Portfolio Metrics:\")\n",
    "print(f\"Weights:{final_allocations}, Shares:{LSTM_shares}\")\n",
    "print(metrics)\n",
    "\n",
    "model3 = Model()\n",
    "training_portfolio = Portfolio(100000, normalized_train_data)\n",
    "initial_prices = testing_data.loc[testing_data.index[0]]  # First row of the actual price data\n",
    "# Rebalance every 2 years (252 trading days * 2)\n",
    "final_allocations = train_and_rebalance_portfolio(training_portfolio, frequency=20, model=model3)\n",
    "\n",
    "# Step 3: Initialize Testing Portfolio with Final LSTM Weights\n",
    "testing_portfolio = Portfolio(100000, normalized_testing_data)\n",
    "testing_portfolio.rebalance(final_allocations)\n",
    "LSTM_shares = testing_portfolio.calculate_initial_shares(100000,initial_prices)  # Calculate initial shares based on final allocations\n",
    "testing_portfolio.calculate_daily_returns()     # Track performance in testing period\n",
    "testing_portfolio.plot_portfolio_value()        # Visualize portfolio value over time\n",
    "# Step 4: Calculate Performance Metrics for LSTM Portfolio\n",
    "metrics = calculate_metrics(testing_portfolio.portfolio_history)\n",
    "print(\"LSTM Portfolio Metrics:\")\n",
    "print(f\"Weights:{final_allocations}, Shares:{LSTM_shares}\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing clearly suggests a higher rebalancing frequency leads to better results, but with a greater risk of overfitting. I think opting for 30 days between rebalances would increase efficiency and reduce overfitting without sacrificing too much performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute '_Model__build_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m l2_options \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.04\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter optimization\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m best_params, best_sharpe \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Sharpe ratio achieved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_sharpe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 32\u001b[0m, in \u001b[0;36moptimize_hyperparameters\u001b[1;34m(data, units_options, l2_options, n_splits)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Initialize model with current hyperparameters\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_allocations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit on training data\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Predict allocations on validation data\u001b[39;00m\n\u001b[0;32m     35\u001b[0m allocations \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_allocations(val_data)\n",
      "File \u001b[1;32mc:\\Users\\wiztu\\OneDrive\\Documents\\Portfolio Optimisation Capstone\\Model.py:68\u001b[0m, in \u001b[0;36mModel.get_allocations\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mconstant(data), \u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_model(data_w_ret\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m     70\u001b[0m fit_predict_data \u001b[38;5;241m=\u001b[39m data_w_ret[np\u001b[38;5;241m.\u001b[39mnewaxis, :]      \n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(fit_predict_data, np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns))), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute '_Model__build_model'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def optimize_hyperparameters(data, units_options, l2_options, n_splits=5):\n",
    "    \"\"\"\n",
    "    Manually searches over LSTM units and L2 regularization to maximize Sharpe ratio.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame of historical prices.\n",
    "    - units_options: List of LSTM unit values to try.\n",
    "    - l2_options: List of L2 regularization strengths to try.\n",
    "    - n_splits: Number of cross-validation splits.\n",
    "    \n",
    "    Returns:\n",
    "    - best_params: Dictionary of best hyperparameters found.\n",
    "    - best_sharpe: Best Sharpe ratio achieved.\n",
    "    \"\"\"\n",
    "    best_sharpe = -np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    # Create all combinations of hyperparameters\n",
    "    for units, l2_reg in product(units_options, l2_options):\n",
    "        sharpe_ratios = []\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "        for train_index, val_index in tscv.split(data):\n",
    "            train_data, val_data = data.iloc[train_index], data.iloc[val_index]\n",
    "            \n",
    "            # Initialize model with current hyperparameters\n",
    "            model = Model()\n",
    "            model.get_allocations(train_data)  # Fit on training data\n",
    "            \n",
    "            # Predict allocations on validation data\n",
    "            allocations = model.get_allocations(val_data)\n",
    "            \n",
    "            # Calculate portfolio values and returns for validation\n",
    "            portfolio_values = (val_data.values @ allocations)\n",
    "            portfolio_returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "            \n",
    "            # Compute Sharpe ratio for this fold\n",
    "            sharpe_ratio = np.mean(portfolio_returns) / (np.std(portfolio_returns) + 1e-7)\n",
    "            sharpe_ratios.append(sharpe_ratio)\n",
    "        \n",
    "        # Average Sharpe ratio across folds\n",
    "        avg_sharpe = np.mean(sharpe_ratios)\n",
    "        \n",
    "        # Update best parameters if current configuration is better\n",
    "        if avg_sharpe > best_sharpe:\n",
    "            best_sharpe = avg_sharpe\n",
    "            best_params = {'units': units, 'l2': l2_reg}\n",
    "    \n",
    "    return best_params, best_sharpe\n",
    "\n",
    "# Define hyperparameter ranges to search\n",
    "units_options = [32, 64, 128]\n",
    "l2_options = [0.01, 0.04, 0.1]\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "best_params, best_sharpe = optimize_hyperparameters(normalized_data, units_options, l2_options)\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best Sharpe ratio achieved: {best_sharpe}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
