{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete: libraries imported, random seed set, and tickers defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_17820\\1470264350.py:27: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-darkgrid')\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Import necessary libraries for data handling, model utilization, and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf  # For collecting financial data\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.optimize import minimize\n",
    "from collections import deque\n",
    "# Import the custom Model class\n",
    "from Model import Model\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(message)s')\n",
    "\n",
    "# Set the random seed for reproducibility across numpy and tensorflow\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Define the tickers and date range with consideration of trading days\n",
    "TICKERS = ['AGG','DBC','VTI','^VIX']\n",
    "\n",
    "# Approximate number of trading days per year (useful for annualizing returns)\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "VOLATILITY_SCALING = False\n",
    "# Define transaction cost rate\n",
    "C = 0.0001  # 0.01%\n",
    "\n",
    "# Confirm setup\n",
    "print(\"Setup complete: libraries imported, random seed set, and tickers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  4 of 4 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched successfully. Sample data:\n",
      "Ticker            AGG        DBC        VTI   ^VIX\n",
      "Date                                              \n",
      "2006-02-06  56.300117  20.889498  44.654259  13.04\n",
      "2006-02-07  56.260818  20.285255  44.219498  13.59\n",
      "2006-02-08  56.232689  20.198935  44.537624  12.83\n",
      "2006-02-09  56.266430  20.388842  44.452805  13.12\n",
      "2006-02-10  56.148499  20.017660  44.544697  12.87\n",
      "Data covers 3582 trading days with 4 assets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Data Collection Step\n",
    "# Objective: Fetch historical adjusted close prices for defined tickers and date range\n",
    "\n",
    "# Download data using yfinance for the specified tickers and date range\n",
    "def get_data(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Retrieves historical adjusted close prices for the given tickers and date range.\n",
    "    \n",
    "    Parameters:\n",
    "    - tickers: List of stock ticker symbols\n",
    "    - start_date: Start date for historical data\n",
    "    - end_date: End date for historical data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame of adjusted close prices, with each column representing a ticker\n",
    "    \"\"\"\n",
    "    # Fetch data from yfinance\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "    \n",
    "    # Drop rows with missing values, if any, to ensure data continuity\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Fetch the data and display a quick preview\n",
    "data = get_data(TICKERS, '2006-01-01', '2020-04-30')\n",
    "print(\"Data fetched successfully. Sample data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Confirm data spans the expected range and has the expected number of columns\n",
    "print(f\"Data covers {len(data)} trading days with {len(data.columns)} assets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete. Sample normalized data:\n",
      "Ticker           AGG       DBC       VTI      ^VIX\n",
      "Date                                              \n",
      "2006-04-18  1.000000  1.000000  1.000000  1.000000\n",
      "2006-04-19  0.999786  1.001443  1.000798  0.997130\n",
      "2006-04-20  0.999555  1.003226  1.001815  0.993875\n",
      "2006-04-21  0.999332  1.005552  1.002683  0.991806\n",
      "2006-04-24  0.999172  1.007249  1.003528  0.989520\n",
      "\n",
      "Sample daily returns:\n",
      "Ticker           AGG       DBC       VTI      ^VIX\n",
      "Date                                              \n",
      "2006-04-19 -0.000212  0.001441  0.000813 -0.001952\n",
      "2006-04-20 -0.000229  0.001788  0.001035 -0.002230\n",
      "2006-04-21 -0.000221  0.002295  0.000882 -0.001198\n",
      "2006-04-24 -0.000157  0.001709  0.000862 -0.001374\n",
      "2006-04-25 -0.000182  0.002136  0.000757 -0.000993\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def preprocess_data(data, rolling_window=50):\n",
    "    \"\"\"\n",
    "    Prepares data by calculating 50-day rolling averages and returns.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame of historical adjusted close prices for assets\n",
    "    - rolling_window: Window size for the rolling average\n",
    "\n",
    "    Returns:\n",
    "    - normalized_data: Smoothed prices, normalized to start at 1 for each asset\n",
    "    - returns: Smoothed returns using a rolling mean of percentage changes\n",
    "    \"\"\"\n",
    "    # Calculate rolling mean for prices and returns to smooth the data\n",
    "    smoothed_prices = (data.rolling(window=rolling_window).mean()).dropna()\n",
    "    smoothed_returns = (data.pct_change().rolling(window=rolling_window).mean()).dropna()\n",
    "    # Normalize prices to start each asset's time series at 1\n",
    "    normalized_data = smoothed_prices / smoothed_prices.iloc[0]\n",
    "    \n",
    "    return normalized_data, smoothed_returns\n",
    "\n",
    "\n",
    "# Run preprocessing and display sample data\n",
    "normalized_data, smoothed_returns = preprocess_data(data)\n",
    "print(\"Data preprocessing complete. Sample normalized data:\")\n",
    "print(normalized_data.head())\n",
    "print(\"\\nSample daily returns:\")\n",
    "print(smoothed_returns.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class Portfolio:\n",
    "    def __init__(self, initial_cash: float, assets: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the Portfolio object.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_cash: The starting value of the portfolio in cash.\n",
    "        - assets: DataFrame of asset prices (historical data).\n",
    "        \"\"\"\n",
    "        self.initial_cash = initial_cash\n",
    "        self.current_value = initial_cash\n",
    "        self.assets = assets  # Historical price data for the assets\n",
    "        self.weights = np.zeros(len(assets.columns))  # Initialize weights as zero\n",
    "        self.portfolio_history = []  # To track portfolio value over time\n",
    "        self.rebalancing_dates = []  # To store rebalancing dates\n",
    "\n",
    "    def rebalance(self, new_weights: np.array, target_volatility = None):\n",
    "        \"\"\"\n",
    "        Rebalances the portfolio according to new weights.\n",
    "\n",
    "        Parameters:\n",
    "        - new_weights: Numpy array of asset allocations.\n",
    "        \"\"\"\n",
    "        if len(new_weights) != len(self.assets.columns):\n",
    "            raise ValueError(\"Number of weights must match the number of assets.\")\n",
    "        self.weights = new_weights\n",
    "        if target_volatility is not None:\n",
    "            self.apply_volatility_scaling(target_volatility=target_volatility,rolling_window = 50)\n",
    "\n",
    "    def calculate_initial_shares(self, initial_cash, initial_prices):\n",
    "        \"\"\"\n",
    "        Calculates the number of shares for each asset at the start of the testing period based on\n",
    "        initial cash and allocation weights.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_cash: The starting cash value of the portfolio.\n",
    "\n",
    "        Returns:\n",
    "        - shares: Dictionary with tickers as keys and the initial number of shares as values.\n",
    "        \"\"\"\n",
    "        # Calculate the dollar amount allocated to each asset\n",
    "        dollar_allocation = initial_cash * self.weights\n",
    "\n",
    "        # Calculate the number of shares for each asset\n",
    "        shares = (dollar_allocation // initial_prices).astype(int)  # Floor division to get whole shares\n",
    "\n",
    "        # Return as a dictionary for easy readability\n",
    "        return dict(zip(self.assets.columns, shares))\n",
    "    \n",
    "    def calculate_daily_returns(self):\n",
    "        \"\"\"\n",
    "        Applies the current weights to asset returns and updates portfolio value over time.\n",
    "        \"\"\"\n",
    "        # Calculate daily returns for each asset\n",
    "        daily_returns = self.assets.pct_change().dropna()\n",
    "        \n",
    "        # Calculate portfolio returns by applying weights\n",
    "        portfolio_returns = np.dot(daily_returns, self.weights)\n",
    "\n",
    "        # Track the portfolio's value over time by compounding the returns\n",
    "        for daily_ret in portfolio_returns:\n",
    "            self.current_value *= (1 + daily_ret)\n",
    "            self.portfolio_history.append(self.current_value)\n",
    "\n",
    "    def update_portfolio_value(self, date):\n",
    "        \"\"\"\n",
    "        Updates the portfolio value for a single date.\n",
    "        \"\"\"\n",
    "        # Get the index of the date\n",
    "        date_index = self.assets.index.get_loc(date)\n",
    "        if date_index == 0:\n",
    "            # First day, no previous day to compute return\n",
    "            self.portfolio_history.append(self.current_value)\n",
    "            return\n",
    "        # Get the asset returns for that day\n",
    "        previous_date = self.assets.index[date_index - 1]\n",
    "        daily_return = self.assets.loc[date] / self.assets.loc[previous_date] - 1\n",
    "        # Calculate portfolio return\n",
    "        portfolio_return = np.dot(daily_return.values, self.weights)\n",
    "        # Update portfolio value\n",
    "        self.current_value *= (1 + portfolio_return)\n",
    "        # Append to history\n",
    "        self.portfolio_history.append(self.current_value)\n",
    "\n",
    "\n",
    "    def track_portfolio_performance(self):\n",
    "        \"\"\"\n",
    "        Tracks and prints the portfolio performance over time.\n",
    "        \"\"\"\n",
    "        for date, value in zip(self.assets.index[1:], self.portfolio_history):\n",
    "            print(f\"Date: {date}, Portfolio Value: {value}\")\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_cash = self.initial_cash\n",
    "        self.assets = self.initial_assets.copy()\n",
    "        self.weights = np.zeros(len(self.assets.columns))  # Reset to no investments\n",
    "        self.portfolio_history = []\n",
    "        self.rebalancing_dates = []\n",
    "        return self.assets.iloc[0].values\n",
    "\n",
    "    def get_portfolio_value(self):\n",
    "        \"\"\"\n",
    "        Returns the current value of the portfolio.\n",
    "        \"\"\"\n",
    "        return self.current_value\n",
    "    \n",
    "    def plot_portfolio_value(self):\n",
    "        \"\"\"\n",
    "        Plots the portfolio value over time.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.assets.index[1:], self.portfolio_history, label=\"Portfolio Value\")\n",
    "        plt.title(\"Portfolio Value Over Time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Portfolio Value\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def calculate_metrics(portfolio_values):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics for the portfolio.\n",
    "\n",
    "    Parameters:\n",
    "    - portfolio_values: List of daily portfolio values over the testing period.\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary containing various performance metrics.\n",
    "    \"\"\"\n",
    "    # Convert portfolio values to daily returns\n",
    "    portfolio_returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "    \n",
    "    # Number of days\n",
    "    N = len(portfolio_returns)\n",
    "\n",
    "    # Calculate Sharpe Ratio\n",
    "    mean_return = np.mean(portfolio_returns)\n",
    "    std_dev = np.std(portfolio_returns)\n",
    "    sharpe_ratio = mean_return / std_dev * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    \n",
    "    # Calculate Sortino Ratio\n",
    "    downside_returns = portfolio_returns[portfolio_returns < 0]\n",
    "    downside_std_dev = np.std(downside_returns) if len(downside_returns) > 0 else 0\n",
    "    sortino_ratio = mean_return / downside_std_dev * np.sqrt(TRADING_DAYS_PER_YEAR) if downside_std_dev != 0 else np.nan\n",
    "    \n",
    "    # Calculate Maximum Drawdown\n",
    "    cumulative_max = np.maximum.accumulate(portfolio_values)\n",
    "    drawdowns = (cumulative_max - portfolio_values) / cumulative_max\n",
    "    max_drawdown = np.max(drawdowns)\n",
    "    \n",
    "    # Expected return (annualized)\n",
    "    cumulative_return = (portfolio_values[-1] / portfolio_values[0]) - 1\n",
    "    annualized_return = (1 + cumulative_return) ** (TRADING_DAYS_PER_YEAR / N) - 1\n",
    "\n",
    "    # Standard deviation of returns (annualized)\n",
    "    annualized_std = std_dev * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "\n",
    "    # Percentage of positive returns\n",
    "    positive_returns = portfolio_returns[portfolio_returns > 0]\n",
    "    percentage_positive = len(positive_returns) / len(portfolio_returns) * 100\n",
    "\n",
    "    # Average profit / average loss (profit/loss ratio)\n",
    "    average_profit = np.mean(portfolio_returns[portfolio_returns > 0]) if len(positive_returns) > 0 else 0\n",
    "    average_loss = np.mean(portfolio_returns[portfolio_returns < 0]) if len(portfolio_returns[portfolio_returns < 0]) > 0 else 0\n",
    "    profit_loss_ratio = (average_profit / -average_loss) if average_loss != 0 else np.nan\n",
    "\n",
    "    metrics = {\n",
    "        \"Annualized Return\": annualized_return,\n",
    "        \"Annualized Std Dev\": annualized_std,\n",
    "        \"Sharpe Ratio\": sharpe_ratio,\n",
    "        \"Sortino Ratio\": sortino_ratio,\n",
    "        \"Max Drawdown\": max_drawdown,\n",
    "        \"% Positive Returns\": percentage_positive,\n",
    "        \"Profit/Loss Ratio\": profit_loss_ratio\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volatility_scaling(weights, returns_past, target_vol=0.1):\n",
    "    \"\"\"\n",
    "    Scales the portfolio weights based on the target volatility and ex-ante volatility estimates.\n",
    "\n",
    "    Parameters:\n",
    "    - weights: Numpy array of current portfolio weights\n",
    "    - returns_past: Pandas DataFrame of past returns (50 days)\n",
    "    - target_vol: Target portfolio volatility (default 10%)\n",
    "\n",
    "    Returns:\n",
    "    - scaled_weights: Numpy array of scaled portfolio weights\n",
    "    \"\"\"\n",
    "    # Calculate exponentially weighted moving standard deviation (50-day window)\n",
    "    vol_estimates = returns_past.ewm(span=50).std().iloc[-1].values  # Shape: (num_assets,)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    vol_estimates = np.where(vol_estimates == 0, 1e-6, vol_estimates)\n",
    "\n",
    "    # Calculate scaling factors: target_vol / vol_estimates\n",
    "    scaling_factors = target_vol / vol_estimates\n",
    "\n",
    "    # Apply scaling to weights\n",
    "    scaled_weights = weights * scaling_factors\n",
    "\n",
    "    # Normalize weights to sum to 1\n",
    "    scaled_weights /= np.sum(scaled_weights)\n",
    "\n",
    "    return scaled_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def equal_weighted_strategy(returns):\n",
    "    \"\"\"\n",
    "    Creates an equal-weighted portfolio.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: DataFrame of daily returns for each asset.\n",
    "\n",
    "    Returns:\n",
    "    - equal_weights: Numpy array of equal weights for each asset.\n",
    "    \"\"\"\n",
    "    num_assets = returns.shape[1]\n",
    "    equal_weights = np.ones(num_assets) / num_assets\n",
    "    return equal_weights\n",
    "# Define function to get MV weights\n",
    "def mean_variance_optimized_strategy(returns):\n",
    "    \"\"\"\n",
    "    Creates a mean-variance optimized portfolio by maximizing the Sharpe Ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: DataFrame of daily returns for each asset.\n",
    "\n",
    "    Returns:\n",
    "    - optimized_weights: Numpy array of optimized weights for each asset.\n",
    "    \"\"\"\n",
    "    mean_returns = returns.mean()\n",
    "    cov_matrix = returns.cov()\n",
    "    \n",
    "    def neg_sharpe(weights):\n",
    "        portfolio_return = np.dot(weights, mean_returns)\n",
    "        portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        return -portfolio_return / portfolio_std\n",
    "\n",
    "    # Constraints: Weights must sum to 1, and each weight must be between 0 and 1\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(returns.shape[1]))\n",
    "\n",
    "    result = minimize(neg_sharpe, np.ones(returns.shape[1]) / returns.shape[1], bounds=bounds, constraints=constraints)\n",
    "    optimized_weights = result.x\n",
    "    \n",
    "    return optimized_weights\n",
    "\n",
    "# Define function to get MD weights\n",
    "def maximum_diversification(returns):\n",
    "    \"\"\"\n",
    "    Perform maximum diversification optimization based on the given returns.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: DataFrame of daily returns for each asset.\n",
    "\n",
    "    Returns:\n",
    "    - optimal_weights: Array of portfolio weights that maximize diversification.\n",
    "    \"\"\"\n",
    "    # Calculate asset volatilities (standard deviation of each assetâ€™s returns)\n",
    "    asset_volatilities = returns.std()\n",
    "\n",
    "    # Calculate the covariance matrix of returns\n",
    "    cov_matrix = returns.cov()\n",
    "\n",
    "    # Define the diversification ratio to be maximized\n",
    "    def neg_diversification_ratio(weights):\n",
    "        # Calculate the weighted average asset volatility\n",
    "        weighted_volatility = np.dot(weights, asset_volatilities)\n",
    "        \n",
    "        # Calculate the portfolio volatility as the weighted covariance matrix\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        \n",
    "        # Diversification ratio (we negate this because we want to maximize it)\n",
    "        diversification_ratio = weighted_volatility / portfolio_volatility\n",
    "        return -diversification_ratio  # Negate to turn this into a minimization problem\n",
    "\n",
    "    # Constraints: weights sum to 1, and each weight between 0 and 1 (long-only portfolio)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(len(asset_volatilities)))\n",
    "\n",
    "    # Initial guess (equal allocation)\n",
    "    init_guess = np.ones(len(asset_volatilities)) / len(asset_volatilities)\n",
    "\n",
    "    # Optimize to find weights that maximize diversification ratio\n",
    "    result = minimize(neg_diversification_ratio, init_guess, bounds=bounds, constraints=constraints)\n",
    "    optimal_weights = result.x\n",
    "    \n",
    "    return optimal_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(metrics_list):\n",
    "    \"\"\"\n",
    "    Calculates the average of each metric in the list of metrics.\n",
    "    \"\"\"\n",
    "    avg_metrics = {}\n",
    "    keys = metrics_list[0].keys()\n",
    "    for key in keys:\n",
    "        avg_metrics[key] = np.mean([m[key] for m in metrics_list])\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define testing periods\n",
    "training_end_dates = ['2010-12-31', '2012-12-31', '2014-12-31', '2016-12-31', '2018-12-31']\n",
    "testing_start_dates = ['2011-01-01', '2013-01-01', '2015-01-01', '2017-01-01', '2019-01-01']\n",
    "testing_end_dates = ['2012-12-31', '2014-12-31', '2016-12-31', '2018-12-31', '2020-12-31']\n",
    "\n",
    "periods = list(zip(training_end_dates, testing_start_dates, testing_end_dates))\n",
    "\n",
    "# Initialize lists to store performance metrics for each model\n",
    "lstm_metrics = []\n",
    "mvo_metrics = []\n",
    "md_metrics = []\n",
    "TARGET_VOL = 0.1\n",
    "initial_cash = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing period: Training up to 2010-12-31, Testing from 2011-01-01 to 2012-12-31\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,4) and (50,) not aligned: 4 (dim 1) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m sigma \u001b[38;5;241m=\u001b[39m returns_full_testing\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m50\u001b[39m:i]\u001b[38;5;241m.\u001b[39mcov()\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Shape: (num_assets, num_assets)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Compute MV weights using aggregated rolling stats\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m weights_mv \u001b[38;5;241m=\u001b[39m \u001b[43mget_mv_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Apply volatility scaling if required\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m VOLATILITY_SCALING:\n",
      "Cell \u001b[1;32mIn[7], line 47\u001b[0m, in \u001b[0;36mget_mv_weights\u001b[1;34m(mu, sigma, risk_free_rate)\u001b[0m\n\u001b[0;32m     44\u001b[0m initial_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(num_assets) \u001b[38;5;241m/\u001b[39m num_assets\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Optimize\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_sharpe_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Ensure the solution is valid\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\optimize\\_minimize.py:705\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    702\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    703\u001b[0m                             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 705\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m    706\u001b[0m                           constraints, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    708\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    709\u001b[0m                                        bounds, constraints,\n\u001b[0;32m    710\u001b[0m                                        callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\optimize\\_slsqp_py.py:374\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    371\u001b[0m     xu[infbnd[:, \u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# ScalarFunction provides function and gradient evaluation\u001b[39;00m\n\u001b[1;32m--> 374\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# gh11403 SLSQP sometimes exceeds bounds by 1 or 2 ULP, make sure this\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# doesn't get sent to the func/grad evaluator.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m wrapped_fun \u001b[38;5;241m=\u001b[39m _clip_x_for_func(sf\u001b[38;5;241m.\u001b[39mfun, new_bounds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\optimize\\_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    328\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m, in \u001b[0;36mget_mv_weights.<locals>.neg_sharpe_ratio\u001b[1;34m(weights)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mneg_sharpe_ratio\u001b[39m(weights):\n\u001b[0;32m     32\u001b[0m     port_return \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(weights, mu)\n\u001b[1;32m---> 33\u001b[0m     port_vol \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdot(weights\u001b[38;5;241m.\u001b[39mT, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     34\u001b[0m     sharpe_ratio \u001b[38;5;241m=\u001b[39m (port_return \u001b[38;5;241m-\u001b[39m risk_free_rate) \u001b[38;5;241m/\u001b[39m (port_vol \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39msharpe_ratio\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,4) and (50,) not aligned: 4 (dim 1) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "for period in periods:\n",
    "    training_end, testing_start, testing_end = period\n",
    "    print(f\"\\nProcessing period: Training up to {training_end}, Testing from {testing_start} to {testing_end}\")\n",
    "\n",
    "    # Get training data\n",
    "    training_data = data.loc[:training_end].copy()\n",
    "    # Get testing data\n",
    "    testing_data = data.loc[testing_start:testing_end].copy()\n",
    "    testing_returns = testing_data.pct_change().fillna(0)\n",
    "    # Ensure we have enough data\n",
    "    if len(testing_data) < 50:\n",
    "        print(\"Not enough data for testing period\")\n",
    "        continue\n",
    "\n",
    "    # LSTM Model\n",
    "    # lstm_model = Model()\n",
    "    # # Train the model on the training data\n",
    "    # lstm_model.train(training_data)\n",
    "\n",
    "    # # Initialize portfolio values\n",
    "    # portfolio_values_lstm = [initial_cash]\n",
    "    # prev_weights_lstm = None\n",
    "\n",
    "    # # Loop through each day in the testing period for LSTM\n",
    "    # for i in range(50, len(testing_data)):\n",
    "    #     # Get past 50 days of data for input\n",
    "    #     input_sequence = testing_data.iloc[i - 50:i]\n",
    "    #     # Prepare the input (same preprocessing as during training)\n",
    "    #     returns_sequence = input_sequence.pct_change().fillna(0)\n",
    "    #     combined_sequence = pd.concat([input_sequence, returns_sequence], axis=1).values\n",
    "\n",
    "    #     # Predict allocation for the day\n",
    "    #     allocation = lstm_model.predict_allocation(combined_sequence)\n",
    "    #     print(\"LSTM Allocation:\", allocation)\n",
    "    #     # Apply volatility scaling if needed\n",
    "    #     if VOLATILITY_SCALING:\n",
    "    #         # Get past returns for volatility estimation\n",
    "    #         returns_past = testing_returns.iloc[i - 50:i]\n",
    "    #         allocation = volatility_scaling(allocation, returns_past, target_vol=TARGET_VOL)\n",
    "\n",
    "    #     # Get today's returns from precomputed returns\n",
    "    #     today_return = testing_returns.iloc[i].values\n",
    "\n",
    "    #     # Compute transaction costs\n",
    "    #     if prev_weights_lstm is None:\n",
    "    #         transaction_cost = C * np.sum(np.abs(allocation))\n",
    "    #     else:\n",
    "    #         transaction_cost = C * np.sum(np.abs(allocation - prev_weights_lstm))\n",
    "\n",
    "    #     # Update portfolio value\n",
    "    #     portfolio_return = np.dot(allocation, today_return)\n",
    "    #     new_value = portfolio_values_lstm[-1] * (1 + portfolio_return) - transaction_cost * portfolio_values_lstm[-1]\n",
    "    #     portfolio_values_lstm.append(new_value)\n",
    "\n",
    "    #     # Update previous weights\n",
    "    #     prev_weights_lstm = allocation\n",
    "\n",
    "    # # Calculate and store performance metrics for LSTM\n",
    "    # metrics_lstm = calculate_metrics(portfolio_values_lstm)\n",
    "    # lstm_metrics.append(metrics_lstm)\n",
    "\n",
    "    # Mean-Variance Optimization (MVO) Model\n",
    "    # Precompute rolling metrics (done outside loop for efficiency if possible)\n",
    "    # Here it's recomputed per period, which may not be necessary\n",
    "    returns_full_testing = data.loc[:testing_end].pct_change().dropna()\n",
    "    returns_testing = data.loc[testing_start:testing_end].pct_change().dropna()\n",
    "\n",
    "    # Get indices of testing dates in returns\n",
    "    testing_indices = returns_full_testing.index.get_indexer_for(returns_testing.index)\n",
    "\n",
    "    portfolio_values_mv = [initial_cash]\n",
    "    prev_weights_mv = None\n",
    "\n",
    "    for i in range(50, len(testing_indices)):  # Start after the first 50 days of testing\n",
    "        # Get the past 50 days of rolling means\n",
    "        input_data = returns_full_testing.iloc[i - 50:i]\n",
    "        weights_mv = mean_variance_optimized_strategy(input_data)\n",
    "\n",
    "        # Apply volatility scaling if required\n",
    "        if VOLATILITY_SCALING:\n",
    "            weights_mv_scaled = volatility_scaling(weights_mv, returns_full_testing.iloc[i - 50:i], target_vol=TARGET_VOL)\n",
    "        else:\n",
    "            weights_mv_scaled = weights_mv\n",
    "\n",
    "        print(\"MVO Allocation:\", weights_mv_scaled)\n",
    "        # Today's returns\n",
    "        today_return = returns_testing.iloc[i].values\n",
    "\n",
    "        # Compute transaction costs\n",
    "        if prev_weights_mv is None:\n",
    "            transaction_cost = C * np.sum(np.abs(weights_mv_scaled))\n",
    "        else:\n",
    "            transaction_cost = C * np.sum(np.abs(weights_mv_scaled - prev_weights_mv))\n",
    "\n",
    "        # Update portfolio value\n",
    "        portfolio_return = np.dot(weights_mv_scaled, today_return)\n",
    "        portfolio_value = portfolio_values_mv[-1] * (1 + portfolio_return) - transaction_cost * portfolio_values_mv[-1]\n",
    "        portfolio_values_mv.append(portfolio_value)\n",
    "\n",
    "        # Update previous weights\n",
    "        prev_weights_mv = weights_mv_scaled\n",
    "\n",
    "    # Calculate and store performance metrics for MVO\n",
    "    metrics_mv = calculate_metrics(portfolio_values_mv)\n",
    "    mvo_metrics.append(metrics_mv)\n",
    "\n",
    "    # Maximum Diversification (MD) Model\n",
    "    portfolio_values_md = [initial_cash]\n",
    "    prev_weights_md = None\n",
    "\n",
    "    for i in range(50, len(testing_indices)):  # Start after the first 50 days of testing\n",
    "        input_data = returns_full_testing.iloc[i - 50:i]\n",
    "        weights_md = maximum_diversification(input_data)\n",
    "        # Apply volatility scaling if required\n",
    "        if VOLATILITY_SCALING:\n",
    "            weights_md_scaled = volatility_scaling(weights_md, returns_full_testing.iloc[i - 50:i], target_vol=TARGET_VOL)\n",
    "        else:\n",
    "            weights_md_scaled = weights_md\n",
    "\n",
    "        print(\"MD Allocation:\", weights_md_scaled)\n",
    "        # Today's returns\n",
    "        today_return = returns_testing.iloc[i].values\n",
    "\n",
    "        # Compute transaction costs\n",
    "        if prev_weights_md is None:\n",
    "            transaction_cost = C * np.sum(np.abs(weights_md_scaled))\n",
    "        else:\n",
    "            transaction_cost = C * np.sum(np.abs(weights_md_scaled - prev_weights_md))\n",
    "\n",
    "        # Update portfolio value\n",
    "        portfolio_return = np.dot(weights_md_scaled, today_return)\n",
    "        portfolio_value = portfolio_values_md[-1] * (1 + portfolio_return) - transaction_cost * portfolio_values_md[-1]\n",
    "        portfolio_values_md.append(portfolio_value)\n",
    "\n",
    "        # Update previous weights\n",
    "        prev_weights_md = weights_md_scaled\n",
    "\n",
    "    # Calculate and store performance metrics for MD\n",
    "    metrics_md = calculate_metrics(portfolio_values_md)\n",
    "    md_metrics.append(metrics_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics for each model\n",
    "# lstm_avg_metrics = average_metrics(lstm_metrics)\n",
    "mvo_avg_metrics = average_metrics(mvo_metrics)\n",
    "md_avg_metrics = average_metrics(md_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metrics for MVO Strategy:\n",
      "Annualized Return: 0.0930\n",
      "Annualized Std Dev: 0.0868\n",
      "Sharpe Ratio: 1.2788\n",
      "Sortino Ratio: 1.9150\n",
      "Max Drawdown: 0.0826\n",
      "% Positive Returns: 54.9369\n",
      "Profit/Loss Ratio: 1.0598\n",
      "\n",
      "Average Metrics for MD Strategy:\n",
      "Annualized Return: 0.1392\n",
      "Annualized Std Dev: 0.0902\n",
      "Sharpe Ratio: 1.8799\n",
      "Sortino Ratio: 3.0918\n",
      "Max Drawdown: 0.0777\n",
      "% Positive Returns: 55.6681\n",
      "Profit/Loss Ratio: 1.1472\n"
     ]
    }
   ],
   "source": [
    "# # Print the average metrics\n",
    "# print(\"\\nAverage Metrics for LSTM Model:\")\n",
    "# for key, value in lstm_avg_metrics.items():\n",
    "#     print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Metrics for MVO Strategy:\")\n",
    "for key, value in mvo_avg_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Metrics for MD Strategy:\")\n",
    "for key, value in md_avg_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
